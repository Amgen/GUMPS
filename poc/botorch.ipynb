{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wheymann/mambaforge/envs/gumps_310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.utils.transforms import standardize, normalize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gumps.studies.batch_study import AbstractBatchStudy\n",
    "\n",
    "from gumps.solvers.sampler import SamplerSolverParameters\n",
    "from gumps.apps.parametric_sweep import ParametricSweepApp\n",
    "\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a batch study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_variables = {}\n",
    "\n",
    "class MultiBatch(AbstractBatchStudy):\n",
    "    \"batch version of sphere study (designed to approximate surrogate model)\"\n",
    "\n",
    "    def __init__(self, model_variables:dict):\n",
    "        self.model_variables = model_variables\n",
    "\n",
    "    def start(self):\n",
    "        \"initialize this study\"\n",
    "\n",
    "    def stop(self):\n",
    "        \"handle shutdown tasks\"\n",
    "\n",
    "    def run(self, input_data:pd.DataFrame, processing_function) -> pd.DataFrame:\n",
    "        \"run the batch simulation\"\n",
    "        diff = pd.DataFrame({'d1': input_data['a']**2 + input_data['b']**2,\n",
    "                            'd2': input_data['b']**2 + input_data['c']**2,\n",
    "                            'd3': input_data['c']**2 + input_data['a']**2,\n",
    "                            'd4': input_data['a']**2 + input_data['b']**2 + input_data['c']**2})\n",
    "        self.save_results(input_data, diff)\n",
    "\n",
    "        return processing_function(diff)\n",
    "\n",
    "batch = MultiBatch(model_variables=model_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample the batch study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = SamplerSolverParameters(\n",
    "    number_of_samples = 128,\n",
    "    lower_bound = {'a':-10, 'b':-10, 'c':-10},\n",
    "    upper_bound = {'a':10,  'b':10, 'c':10},\n",
    "    sampler = \"sobol\"\n",
    "    )\n",
    "\n",
    "def get_all(frame:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"processing function to get the total from the dataframe\"\n",
    "    return frame\n",
    "\n",
    "app = ParametricSweepApp(parameters=parameters,\n",
    "        processing_function=get_all,\n",
    "        pre_processing_function=None,\n",
    "        directory=None,\n",
    "        batch=batch)\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SumMarginalLogLikelihood(\n",
       "  (likelihood): LikelihoodList(\n",
       "    (likelihoods): ModuleList(\n",
       "      (0-3): 4 x GaussianLikelihood(\n",
       "        (noise_covar): HomoskedasticNoise(\n",
       "          (noise_prior): GammaPrior()\n",
       "          (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model): ModelListGP(\n",
       "    (models): ModuleList(\n",
       "      (0-3): 4 x SingleTaskGP(\n",
       "        (likelihood): GaussianLikelihood(\n",
       "          (noise_covar): HomoskedasticNoise(\n",
       "            (noise_prior): GammaPrior()\n",
       "            (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "          )\n",
       "        )\n",
       "        (mean_module): ConstantMean()\n",
       "        (covar_module): ScaleKernel(\n",
       "          (base_kernel): MaternKernel(\n",
       "            (lengthscale_prior): GammaPrior()\n",
       "            (raw_lengthscale_constraint): Positive()\n",
       "          )\n",
       "          (outputscale_prior): GammaPrior()\n",
       "          (raw_outputscale_constraint): Positive()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (likelihood): LikelihoodList(\n",
       "      (likelihoods): ModuleList(\n",
       "        (0-3): 4 x GaussianLikelihood(\n",
       "          (noise_covar): HomoskedasticNoise(\n",
       "            (noise_prior): GammaPrior()\n",
       "            (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlls): ModuleList(\n",
       "    (0-3): 4 x ExactMarginalLogLikelihood(\n",
       "      (likelihood): GaussianLikelihood(\n",
       "        (noise_covar): HomoskedasticNoise(\n",
       "          (noise_prior): GammaPrior()\n",
       "          (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "        )\n",
       "      )\n",
       "      (model): SingleTaskGP(\n",
       "        (likelihood): GaussianLikelihood(\n",
       "          (noise_covar): HomoskedasticNoise(\n",
       "            (noise_prior): GammaPrior()\n",
       "            (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "          )\n",
       "        )\n",
       "        (mean_module): ConstantMean()\n",
       "        (covar_module): ScaleKernel(\n",
       "          (base_kernel): MaternKernel(\n",
       "            (lengthscale_prior): GammaPrior()\n",
       "            (raw_lengthscale_constraint): Positive()\n",
       "          )\n",
       "          (outputscale_prior): GammaPrior()\n",
       "          (raw_outputscale_constraint): Positive()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "output_scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "train_X = input_scaler.fit_transform(app.factors.to_numpy())\n",
    "train_Y = output_scaler.fit_transform(app.responses.to_numpy())\n",
    "\n",
    "train_X = torch.DoubleTensor(train_X)\n",
    "train_Y = torch.DoubleTensor(train_Y)\n",
    "\n",
    "\n",
    "models = []\n",
    "for i in range(train_Y.shape[-1]):\n",
    "    models.append(\n",
    "        SingleTaskGP(\n",
    "            train_X, train_Y[..., i : i + 1]\n",
    "        )\n",
    "    )\n",
    "gp = ModelListGP(*models)\n",
    "mll = SumMarginalLogLikelihood(gp.likelihood, gp)\n",
    "\n",
    "fit_gpytorch_mll(mll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedError",
     "evalue": "Must specify a posterior transform when using a multi-output model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/wheymann/GUMPS/poc/botorch.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/wheymann/GUMPS/poc/botorch.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbotorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39macquisition\u001b[39;00m \u001b[39mimport\u001b[39;00m UpperConfidenceBound\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/wheymann/GUMPS/poc/botorch.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m UCB \u001b[39m=\u001b[39m UpperConfidenceBound(gp, beta\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, maximize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/mambaforge/envs/gumps_310/lib/python3.10/site-packages/botorch/acquisition/analytic.py:760\u001b[0m, in \u001b[0;36mUpperConfidenceBound.__init__\u001b[0;34m(self, model, beta, posterior_transform, maximize)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    742\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    743\u001b[0m     model: Model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    746\u001b[0m     maximize: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    747\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Single-outcome Upper Confidence Bound.\u001b[39;00m\n\u001b[1;32m    749\u001b[0m \n\u001b[1;32m    750\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[39m        maximize: If True, consider the problem a maximization problem.\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 760\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model\u001b[39m=\u001b[39;49mmodel, posterior_transform\u001b[39m=\u001b[39;49mposterior_transform)\n\u001b[1;32m    761\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\u001b[39m\"\u001b[39m\u001b[39mbeta\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mas_tensor(beta))\n\u001b[1;32m    762\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaximize \u001b[39m=\u001b[39m maximize\n",
      "File \u001b[0;32m~/mambaforge/envs/gumps_310/lib/python3.10/site-packages/botorch/acquisition/analytic.py:72\u001b[0m, in \u001b[0;36mAnalyticAcquisitionFunction.__init__\u001b[0;34m(self, model, posterior_transform)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m posterior_transform \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mnum_outputs \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m         \u001b[39mraise\u001b[39;00m UnsupportedError(\n\u001b[1;32m     73\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMust specify a posterior transform when using a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmulti-output model.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m         )\n\u001b[1;32m     76\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(posterior_transform, PosteriorTransform):\n",
      "\u001b[0;31mUnsupportedError\u001b[0m: Must specify a posterior transform when using a multi-output model."
     ]
    }
   ],
   "source": [
    "from botorch.acquisition import UpperConfidenceBound\n",
    "\n",
    "UCB = UpperConfidenceBound(gp, beta=0.1, maximize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5069, 0.5090, 0.4701, 0.3914, 0.4506]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "bounds = torch.stack([torch.zeros(5), torch.ones(5)])\n",
    "candidate, acq_value = optimize_acqf(\n",
    "    UCB, bounds=bounds, q=1, num_restarts=10, raw_samples=50\n",
    ")\n",
    "candidate  # tensor([0.4887, 0.5063])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18668982,  0.3207016 , 19.466476  ,  0.06941879,  4.5460224 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_scaler.inverse_transform(candidate.detach().numpy().reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gumps_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
